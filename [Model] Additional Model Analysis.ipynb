{"cells":[{"cell_type":"code","execution_count":null,"id":"4a93076f-f529-432a-a73d-16332136636d","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"4a93076f-f529-432a-a73d-16332136636d","outputId":"3b86a3ca-f231-46cb-c842-13fb914a4d93","scrolled":true,"executionInfo":{"status":"error","timestamp":1714936890946,"user_tz":-60,"elapsed":2327,"user":{"displayName":"Patricio J.P.","userId":"10801288192480613531"}}},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'df_3.csv'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-16c635b497d7>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Read the CSV file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'df_3.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Replace 'Unavailable' and 'unknown' values with NaN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'df_3.csv'"]}],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.preprocessing import StandardScaler\n","from scipy.stats import skew\n","\n","# Missing Values\n","# Set display options to show more columns\n","pd.set_option('display.max_columns', None)\n","pd.set_option('display.max_rows', None)\n","\n","# Read the CSV file\n","df = pd.read_csv('df_3.csv')\n","\n","# Replace 'Unavailable' and 'unknown' values with NaN\n","df = df.replace('Unavailable', pd.NA)\n","df = df.replace('Unknown', pd.NA)\n","\n","# Calculate the number of missing values in each column\n","null_counts = df.isna().sum()\n","\n","# Output the number of missing values for each column\n","print(null_counts)\n","print(len(null_counts))\n"]},{"cell_type":"code","execution_count":null,"id":"84446ba4-7242-495b-9452-a35cb297fe3e","metadata":{"id":"84446ba4-7242-495b-9452-a35cb297fe3e","scrolled":true},"outputs":[],"source":["\n","# Get the total number of rows\n","total_rows = len(df)\n","\n","# Find columns with less than 2% missing values\n","columns_with_few_nulls = null_counts[null_counts / total_rows < 0.02].index\n","print(len(columns_with_few_nulls))\n","\n","# Create a new DataFrame containing only columns with less than 2% missing values, and remove unnecessary columns\n","new_df = df[columns_with_few_nulls]\n","new_null_counts = new_df.isna().sum()\n","print(new_null_counts)\n","\n"]},{"cell_type":"code","execution_count":null,"id":"53c3ff56-f454-4c43-b176-14fc1b1db93f","metadata":{"id":"53c3ff56-f454-4c43-b176-14fc1b1db93f"},"outputs":[],"source":["columns_to_drop = [\n","    \"Country Iso Code\",\n","    \"Region\",\n","    \"Year\",\n","    \"Summary Paragraph\",\n","    \"Assessment_Level\"\n","]\n","\n","new_df = new_df.drop(columns=columns_to_drop)\n","new_df.to_csv('new_df.csv', index=False)\n"]},{"cell_type":"code","execution_count":null,"id":"9f96eb05-5967-4d52-99c5-770bc52a64ef","metadata":{"id":"9f96eb05-5967-4d52-99c5-770bc52a64ef"},"outputs":[],"source":["new_df.info()"]},{"cell_type":"code","execution_count":null,"id":"4b5a28d4-fe1e-4a89-a94e-d70e25dc241c","metadata":{"id":"4b5a28d4-fe1e-4a89-a94e-d70e25dc241c"},"outputs":[],"source":["# Convert 'Percent Of Working Children' column to numeric type\n","new_df['Percent Of Working Children'] = pd.to_numeric(new_df['Percent Of Working Children'], errors='coerce')\n","\n","# Remove missing values\n","cleaned_data = new_df['Percent Of Working Children'].dropna()\n","\n","# Plot histogram and Kernel Density Estimate (KDE) to view the distribution\n","plt.figure(figsize=(10, 6))\n","sns.histplot(cleaned_data, kde=True)\n","plt.title('Distribution of Percent Of Working Children (excluding missing values)')\n","plt.xlabel('Percent Of Working Children')\n","plt.ylabel('Frequency')\n","plt.show()\n","\n","# Calculate mean and median\n","mean_value = cleaned_data.mean()\n","median_value = cleaned_data.median()\n","\n","print(f\"Mean: {mean_value}\")\n","print(f\"Median: {median_value}\")\n","\n","# Decide which value to use for imputation\n","if cleaned_data.skew() > 1 or cleaned_data.skew() < -1:\n","    print(\"The distribution is significantly skewed. Consider using median for imputation.\")\n","    imputed_value = median_value\n","else:\n","    print(\"The distribution is not significantly skewed. Consider using mean for imputation.\")\n","    imputed_value = mean_value\n","\n","# Impute using the selected value\n","new_df['Percent Of Working Children'] = new_df['Percent Of Working Children'].fillna(imputed_value)\n","\n"]},{"cell_type":"code","execution_count":null,"id":"4f7e9a88-b82a-41d1-aca1-de7b77f98193","metadata":{"id":"4f7e9a88-b82a-41d1-aca1-de7b77f98193","scrolled":true},"outputs":[],"source":["# Check the unique values in each column of new_df\n","unique_values_in_columns = {column: new_df[column].unique() for column in new_df.columns}\n","unique_values_in_columns\n"]},{"cell_type":"code","execution_count":null,"id":"d49fde6d-80b4-4fb1-ac97-4c9cea464284","metadata":{"id":"d49fde6d-80b4-4fb1-ac97-4c9cea464284"},"outputs":[],"source":["# Find columns with only one unique value\n","columns_with_single_value = []\n","for column in new_df.columns:\n","    if len(new_df[column].unique()) == 1:\n","        columns_with_single_value.append(column)\n","print(columns_with_single_value)\n","\n","# Delete these columns\n","new_df.drop(columns=columns_with_single_value, inplace=True)\n","\n","# Display the processed DataFrame to confirm the columns have been correctly removed\n","new_df.head()\n"]},{"cell_type":"code","execution_count":null,"id":"b4d1f31d-c859-4f13-9a8a-cdd4f592c12b","metadata":{"id":"b4d1f31d-c859-4f13-9a8a-cdd4f592c12b","scrolled":true},"outputs":[],"source":["# Special handling for some anomalous data\n","columns_to_clean = ['Enforcement Labor Authorized Assess Penalties', 'Coordination']\n","for column in columns_to_clean:\n","    # Strip whitespace, fill missing values with 'No', and replace empty strings with 'No'\n","    new_df[column] = new_df[column].str.strip().fillna('No').replace({'': 'No'})\n","\n","# '13-Dec' should be '13', likely a recording error\n","new_df['Compulsory Education Age'] = new_df['Compulsory Education Age'].replace('13-Dec', '13')\n","# Convert the 'Compulsory Education Age' column to integer\n","new_df['Compulsory Education Age'] = new_df['Compulsory Education Age'].astype(int)\n","\n","# Check the unique values in each column of new_df after cleaning\n","unique_values_in_columns = {column: new_df[column].unique() for column in new_df.columns}\n","unique_values_in_columns\n"]},{"cell_type":"code","execution_count":null,"id":"1db580bb-d493-4854-b81e-767446c24cf0","metadata":{"id":"1db580bb-d493-4854-b81e-767446c24cf0","scrolled":true},"outputs":[],"source":["# Convert specified columns to integer type\n","integer_columns = [\n","    'Country Numeric Iso Code',\n","    'Year Added',\n","    'Region Id',\n","    'Assessment Level Id',\n","    'Minimum Work Age',\n","    'Minimum Hazardous Work Age',\n","    'Minumum Voluntary Military Age',\n","    'Compulsory Education Age'\n","]\n","\n","for column in integer_columns:\n","    # Convert columns to numeric type first to handle any non-numeric entries and then to 'Int64' to allow for NA values\n","    new_df[column] = pd.to_numeric(new_df[column], errors='coerce').astype('Int64')\n","\n","# Check the unique values in each column of new_df after conversion\n","unique_values_in_columns = {column: new_df[column].unique() for column in new_df.columns}\n","unique_values_in_columns\n"]},{"cell_type":"code","execution_count":null,"id":"5435e4ee-9420-4895-a9dd-3328dee4ed2b","metadata":{"id":"5435e4ee-9420-4895-a9dd-3328dee4ed2b","scrolled":true},"outputs":[],"source":["# Binary variables encoding\n","binary_columns = [\n","    'Crc Csec Ratified',\n","    'Crc Ac Ratified',\n","    'Palermo Ratified',\n","    'Minimum Work Conforms Standard',\n","    'Minimum Hazardous Work Conforms Standard',\n","    'Identification Of Hazardous Occupations List',\n","    'Prohibition Forced Labor Conforms Standard',\n","    'Prohibition Child Trafficking Conforms Standard',\n","    'Prohibition Csec Conforms Standard',\n","    'Prohibition Illicit Activities Conforms Standard',\n","    'Compulsory Education Conforms Standard',\n","    'Free Public Education Conforms Standard',\n","    'Enforcement Labor Authorized Assess Penalties',\n","    'Coordination',\n","    'Policy',\n","    'Program'\n","]\n","\n","for column in binary_columns:\n","    # Replace 'Yes' with 1 and 'No' with 0, then convert to integer\n","    new_df[column] = new_df[column].replace({'Yes': 1, 'No': 0}).astype(int)\n","\n","# Check the unique values in each column of new_df after binary encoding\n","unique_values_in_columns = {column: new_df[column].unique() for column in new_df.columns}\n","unique_values_in_columns\n"]},{"cell_type":"code","execution_count":null,"id":"b6fa7b29-cbbb-4e26-9075-832ab83a3696","metadata":{"id":"b6fa7b29-cbbb-4e26-9075-832ab83a3696"},"outputs":[],"source":["# Encoding age range variables: using the lower limit of the age range for encoding\n","age_range_mapping = {'5 to 14': 5, '6 to 14': 6, '7 to 14': 7, '10 to 14': 10}\n","new_df['Age Range Of Children Attending School'] = new_df['Age Range Of Children Attending School'].map(age_range_mapping)\n","new_df['Age Range Of Children Working And Studying'] = new_df['Age Range Of Children Working And Studying'].map(age_range_mapping)\n","new_df['Age Range Of Working Children'] = new_df['Age Range Of Working Children'].map(age_range_mapping)\n","\n","# Exploitation type encoding: 'DER', 'CL', 'FL', 'FCL' represent different severity levels of exploitation\n","# Assuming higher numbers indicate more severe exploitation\n","exploitation_type_mapping = {'DER': 0, 'CL': 1, 'FL': 0, 'FCL': 2}\n","new_df['Exploitation_Type'] = new_df['Exploitation_Type'].map(exploitation_type_mapping)\n","\n","# Due diligence response encoding should reflect the level of response\n","# Assuming higher numbers indicate higher levels of response\n","due_diligence_response_mapping = {'Basic': 1, 'Heightened': 2, 'Enhanced': 3}\n","new_df['Due_Diligence_Reponse'] = new_df['Due_Diligence_Reponse'].map(due_diligence_response_mapping)\n","\n","# Display the head of the DataFrame to confirm changes\n","new_df.head()\n"]},{"cell_type":"code","execution_count":null,"id":"8a2e53fa-bcbc-4e96-b266-f6b6da428950","metadata":{"id":"8a2e53fa-bcbc-4e96-b266-f6b6da428950"},"outputs":[],"source":["# Perform one-hot encoding on categorical features\n","categorical_columns = ['Sector', 'Good']\n","new_df = pd.get_dummies(new_df, columns=categorical_columns)\n","\n","# Good_AÃÂ§aÃÂ­ Berries rename the unreadable stuff from the new dummies\n","new_df.rename(columns={'Good_AÃÂ§aÃÂ­ Berries': 'Good_Berries'}, inplace=True)\n","\n","# Display the first few rows of the DataFrame to confirm changes\n","new_df.head()\n"]},{"cell_type":"code","execution_count":null,"id":"aae1b549-a62c-41b1-af80-225c96dc48d1","metadata":{"id":"aae1b549-a62c-41b1-af80-225c96dc48d1","scrolled":true},"outputs":[],"source":["unique_values_in_columns = {column: new_df[column].unique() for column in new_df.columns}\n","unique_values_in_columns"]},{"cell_type":"code","execution_count":null,"id":"d650c3a2-e44f-4193-81e5-2697b307b4d1","metadata":{"id":"d650c3a2-e44f-4193-81e5-2697b307b4d1"},"outputs":[],"source":["# Check for missing values in the data\n","null_values = new_df.isnull().sum()\n","print(null_values[null_values > 0])\n"]},{"cell_type":"code","execution_count":null,"id":"E_euOY7OKnla","metadata":{"id":"E_euOY7OKnla"},"outputs":[],"source":["# Encoding True/False Binary Columns\n","new_df[new_df.columns[1:]] = new_df[new_df.columns[1:]].astype('int')\n","\n","new_df.head()"]},{"cell_type":"code","execution_count":null,"id":"0FRbz5IwJ1wy","metadata":{"id":"0FRbz5IwJ1wy"},"outputs":[],"source":["from scipy.stats import chi2_contingency, f_oneway\n","\n","continuous_features  = ['Index_score', 'Legal_framework_score', 'Enforcement_score', 'Outcome_score',\n","                        'Percent Of Working Children',\n","                   'Percent Of Working Children Agriculture',\n","                   'Percent Of Working Children Industry',\n","                   'Percent Of Working Children Services']\n","\n","\n","categorical_features =  [x for x in new_df.drop(columns=['Exploitation_Type', 'Country']).columns if x not in continuous_features]\n","\n","# List to store significant columns\n","significant_columns = []\n","\n","# Catgeorical Feature Selection\n","for column in categorical_features:\n","\n","    # Contingency table for Chi-square test\n","    contingency_table = pd.crosstab(new_df[column], new_df['Exploitation_Type'])\n","\n","    # Perform Chi-square test\n","    chi2_stat, p_val, _, _ = chi2_contingency(contingency_table)\n","\n","    # Reject Columns that aren't significant\n","    if p_val < 0.1:\n","        significant_columns.append(column)\n","\n","\n","# Continuous Feature selection\n","for column in continuous_features:\n","    # Perform ANOVA test\n","    f_stat, p_val_anova = f_oneway(new_df[new_df['Exploitation_Type'] == 0][column], new_df[new_df['Exploitation_Type'] == 1][column], new_df[new_df['Exploitation_Type'] == 2][column])\n","\n","    # Reject Columns that aren't significant\n","    if p_val_anova < 0.1:\n","        significant_columns.append(column)\n"]},{"cell_type":"code","execution_count":null,"id":"nJ6sp3PJRiDp","metadata":{"id":"nJ6sp3PJRiDp"},"outputs":[],"source":["significant_columns"]},{"cell_type":"code","execution_count":null,"id":"3019a991-4492-4cb8-b55b-d775e4c1814d","metadata":{"id":"3019a991-4492-4cb8-b55b-d775e4c1814d"},"outputs":[],"source":["from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import train_test_split\n","from imblearn.over_sampling import SMOTE\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import f1_score\n","\n","x = new_df[significant_columns]\n","y = new_df['Exploitation_Type']\n","\n","# Split the dataset into training and testing sets\n","x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n","\n","# Perform Oversampling\n","smote = SMOTE(random_state=42)\n","x_train_resampled, y_train_resampled = smote.fit_resample(x_train,y_train)\n","\n","# Train the ordinal logistic regression model\n","ordinal_logistic = LogisticRegression(solver='saga', max_iter=5000, C=1, multi_class='multinomial')\n","ordinal_logistic.fit(x_train_resampled, y_train_resampled)\n","\n","# train an SVM model\n","from sklearn.svm import SVC\n","svm = SVC(kernel='rbf', C=0.1, probability=True)\n","svm.fit(x_train_resampled, y_train_resampled)\n","\n","# Train Random Forest\n","from sklearn.ensemble import RandomForestClassifier\n","rf = RandomForestClassifier(n_estimators=50, max_depth=20, random_state=0)\n","rf.fit(x_train_resampled, y_train_resampled)\n","\n","# Train an AdaBoost model\n","from sklearn.ensemble import AdaBoostClassifier\n","ada = AdaBoostClassifier(n_estimators=100, learning_rate=0.01, random_state=0)\n","ada.fit(x_train_resampled, y_train_resampled)\n","\n","# Train a Neural Network\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.utils import to_categorical\n","\n","y_train_resampled_onehot = to_categorical(y_train_resampled, num_classes=3)\n","y_test_onehot = to_categorical(y_test, num_classes=3)\n","\n","# Build the ANN\n","nn = Sequential()\n","nn.add(Dense(100, activation='sigmoid', input_dim=x_train_resampled.shape[1]))\n","nn.add(Dense(100, activation='relu'))\n","nn.add(Dense(100, activation='sigmoid'))\n","nn.add(Dense(100, activation='relu'))\n","nn.add(Dense(100, activation='sigmoid'))\n","nn.add(Dense(100, activation='relu'))\n","nn.add(Dense(3, activation='softmax'))\n","\n","nn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Train the model\n","history = nn.fit(x_train_resampled, y_train_resampled_onehot, epochs=200, batch_size=16, validation_data=(x_test, y_test_onehot), verbose=0)\n","\n","# Accessing the training and validation accuracy\n","train_accuracy = history.history['accuracy'][-1]\n","val_accuracy = history.history['val_accuracy'][-1]\n","\n","# Test the model\n","y_pred_log = ordinal_logistic.predict(x_test)\n","y_tpred_log = ordinal_logistic.predict(x_train)\n","\n","y_pred_svm = svm.predict(x_test)\n","y_tpred_svm = svm.predict(x_train)\n","\n","y_pred_rf = rf.predict(x_test)\n","y_tpred_rf = rf.predict(x_train)\n","\n","y_pred_ada = ada.predict(x_test)\n","y_tpred_ada = ada.predict(x_train)\n","\n","y_pred_nn = nn.predict(x_test)\n","y_tpred_nn = nn.predict(x_train)\n","\n","\n","\n","acc_test1 = accuracy_score(y_test, y_pred_log)\n","acc_train1 = accuracy_score(y_train, y_tpred_log)\n","\n","acc_test2 = accuracy_score(y_test, y_pred_svm)\n","acc_train2 = accuracy_score(y_train, y_tpred_svm)\n","\n","acc_test3 = accuracy_score(y_test, y_pred_rf)\n","acc_train3 = accuracy_score(y_train, y_tpred_rf)\n","\n","acc_test4 = accuracy_score(y_test, y_pred_ada)\n","acc_train4 = accuracy_score(y_train, y_tpred_ada)\n","\n","acc_test5 = val_accuracy\n","acc_train5 = train_accuracy\n","\n","\n","# Evaluation\n","f1_test = f1_score(y_test, y_pred_log, average='weighted')\n","print(\"Ordinal Logistic Regression Test F1:\", f1_test)\n","\n","f1_test = f1_score(y_test, y_pred_svm, average='weighted')\n","print(\"SVM Test F1:\", f1_test)\n","\n","f1_test = f1_score(y_test, y_pred_rf, average='weighted')\n","print(\"Random Forest Test F1:\", f1_test)\n","\n","f1_test = f1_score(y_test, y_pred_ada, average='weighted')\n","print(\"AdaBoost Test F1:\", f1_test)\n","\n","# f1_test = f1_score(y_test, y_pred_nn, average='weighted')\n","# Predict on the test set\n","y_pred_probs = nn.predict(x_test)\n","y_pred_conv = np.argmax(y_pred_probs, axis=1)\n","f1_test = f1_score(y_test, y_pred_conv, average='weighted')  # 'weighted' accounts for label imbalance\n","print(\"Neural Network Test F1:\", f1_test)\n","\n","print(\"\\n----------------------\\n\")\n","## Accuracy\n","print(\"Ordinal Logistic Regression Test Accuracy:\", acc_test1)\n","print(\"SVM Test Accuracy:\", acc_test2)\n","print(\"Random Forest Test Accuracy:\", acc_test3)\n","print(\"AdaBoost Test Accuracy:\", acc_test4)\n","print(\"Neural Network Test Accuracy:\", acc_test5)\n","\n","# Use Ensamble model\n","from sklearn.ensemble import VotingClassifier\n","\n","# Create the ensemble model\n","ensemble = VotingClassifier(estimators=[('lr', ordinal_logistic), ('svm', svm), ('rf', rf), ('ada', ada)], voting='hard')\n","\n","# Fit the model\n","ensemble.fit(x_train_resampled, y_train_resampled)\n","\n","# Evaluate the model\n","y_pred_ens = ensemble.predict(x_test)\n","\n","# Accuracy\n","acc_test_ens = accuracy_score(y_test, y_pred_ens)\n","\n","# Evaluation\n","f1_test_ens = f1_score(y_test, y_pred_ens, average='weighted')\n","\n","print(\"Ensemble Test F1:\", f1_test_ens)\n","print(\"Ensemble Test Accuracy:\", acc_test_ens)\n","\n","\n","# plot training and test accuracy\n","import matplotlib.pyplot as plt\n","\n","# Create a list of model names\n","model_names = ['Ordinal Logistic Regression', 'SVM', 'Random Forest', 'AdaBoost', 'Neural Network']\n","\n","# Create lists for train and test accuracies\n","train_accuracies = [acc_train1, acc_train2, acc_train3, acc_train4, acc_train5]\n","test_accuracies = [acc_test1, acc_test2, acc_test3, acc_test4, acc_test5]\n","\n","# Create a figure and axis object\n","fig, ax = plt.subplots(figsize=(10, 6))\n","\n","# Set the x-axis tick positions and labels\n","x = np.arange(len(model_names))\n","ax.set_xticks(x)\n","ax.set_xticklabels(model_names, rotation=45, ha='right')\n","\n","# Plot the train and test accuracies\n","bar_width = 0.35\n","rects1 = ax.bar(x - bar_width / 2, train_accuracies, bar_width, label='Train Accuracy')\n","rects2 = ax.bar(x + bar_width / 2, test_accuracies, bar_width, label='Test Accuracy')\n","\n","# Add labels and legend\n","ax.set_ylabel('Accuracy')\n","ax.set_title('Train vs Test Accuracy Comparison')\n","ax.legend()\n","\n","# Add value labels on top of the bars\n","for rect1, rect2, model in zip(rects1, rects2, model_names):\n","    ax.text(rect1.get_x() + rect1.get_width() / 2, rect1.get_height() + 0.01, f'{rect1.get_height():.2f}', ha='center', va='bottom')\n","    ax.text(rect2.get_x() + rect2.get_width() / 2, rect2.get_height() + 0.01, f'{rect2.get_height():.2f}', ha='center', va='bottom')\n","\n","# Adjust layout and display the plot\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hRFh7NclLR8t"},"outputs":[],"source":["unique_values, counts = np.unique(y_pred_log, return_counts=True)\n","print(unique_values, counts)\n","#\n","unique_values, counts = np.unique(y_pred_svm, return_counts=True)\n","print(unique_values, counts)\n","#\n","unique_values, counts = np.unique(y_pred_rf, return_counts=True)\n","print(unique_values, counts)\n","#\n","unique_values, counts = np.unique(y_pred_ada, return_counts=True)\n","print(unique_values, counts)\n","#\n","unique_values, counts = np.unique(y_pred_conv, return_counts=True)\n","print(unique_values, counts)\n","\n","unique_values, counts = np.unique(y_test, return_counts=True)\n","print(unique_values, counts)\n"],"id":"hRFh7NclLR8t"},{"cell_type":"code","execution_count":null,"id":"ead7c670-3837-4a7e-b1a1-56afec272d8a","metadata":{"id":"ead7c670-3837-4a7e-b1a1-56afec272d8a","scrolled":true},"outputs":[],"source":["#Weights\n","coefficients = ordinal_logistic.coef_\n","print(coefficients)\n","\n","bias =  ordinal_logistic.intercept_\n","print(bias)\n","\n","classes = ordinal_logistic.classes_\n","print(classes)\n","\n","features = ordinal_logistic.feature_names_in_\n","print(features)\n","\n","# save it in a txt file\n","with open('ordinal_logistic_coefficients.txt', 'w') as f:\n","    for i in range(len(features)):\n","        f.write('Class 0 - ' + features[i] + ':'+ str(coefficients[0][i]) + '\\n')\n","        f.write('Class 1 - ' + features[i] + ':'+ str(coefficients[1][i]) + '\\n')\n","        f.write('Class 2 - ' + features[i] + ':'+ str(coefficients[2][i]) + '\\n')\n","    f.write('biases:' + str(bias[0]) + ', ' + str(bias[1]) + ', '+ str(bias[2]))\n","\n","# find most influential points (5) for each class\n","# Max absolute value of the coefficients for coefficients[0][i]\n","for i in range(len(classes)):\n","    # make coefficients[0][i] all absolute values\n","    abs_coef = [abs(x) for x in coefficients[i]]\n","    total_sum = sum(abs_coef)\n","    # find the top 5 indexes of the absolute values\n","    top_5_indexes = np.argsort(abs_coef)[-5:][::-1]\n","    top_sum = sum(abs(x) for x in coefficients[i][top_5_indexes])\n","    # save the class (0, 1, 2) and the top 5 features with their values\n","    print(classes[i], features[top_5_indexes], coefficients[i][top_5_indexes], top_sum, total_sum)\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"82ee8dc2-2f7e-4944-9ad7-e1382181421c","metadata":{"id":"82ee8dc2-2f7e-4944-9ad7-e1382181421c"},"outputs":[],"source":["# Each Country Risk Score\n","\n","country_risk_probabilities = ordinal_logistic.predict_proba(x)\n","\n","# select from new_df the fields from 'Good_' + whatever text follows regex\n","pattern = r'^Good_'\n","selected_cols_goods = new_df.filter(regex=pattern)\n","goods_cols = selected_cols_goods.columns.tolist()\n","good_names = []\n","\n","# Iterate over rows\n","for idx, row in selected_cols_goods.iterrows():\n","    # Find the column with value 1\n","    good_col = row[row == 1].index.tolist()\n","\n","    # If there is a column with value 1\n","    if good_col:\n","        # Extract the good name from the column name\n","        good_name = good_col[0]\n","\n","        # Remove the Good_ prefix from the good name\n","        good_name = good_name.replace('Good_', '')\n","\n","        # Append the good name to the list\n","        good_names.append(good_name)\n","    else:\n","        # If no column has value 1, append NaN (shouldnt happen...)\n","        good_names.append(np.nan)\n","\n","# Create a new column 'good_name' with the good names\n","selected_cols_goods = pd.DataFrame(good_names)\n","\n","print(selected_cols_goods.shape)\n","print(new_df.shape)\n","\n","country_risk_score = np.dot(country_risk_probabilities, [0,1,2] )\n","\n","probabilities =  [(country, probability, good) for country, probability, good in zip(new_df['Country'], country_risk_probabilities, selected_cols_goods[0])]\n","\n","risks = [(country, risk, good) for country, risk, good in zip(new_df['Country'], country_risk_score, selected_cols_goods[0])]\n","\n","# Sort Lists\n","\n","probability_sorted =sorted(probabilities, key=lambda x: x[0])\n","risk_sorted= sorted(risks, key=lambda x: x[0])\n","\n","\n","print(\"Probabilities: \\n\", probability_sorted)\n","\n","print(\"Risks: \\n\", risk_sorted)"]},{"cell_type":"code","execution_count":null,"id":"rKdiYkVcS20m","metadata":{"id":"rKdiYkVcS20m"},"outputs":[],"source":["for i in probability_sorted:\n","    print(i[0] + ':', i[1])"]},{"cell_type":"code","execution_count":null,"id":"lRigzXXRY_eJ","metadata":{"id":"lRigzXXRY_eJ"},"outputs":[],"source":["for i in risk_sorted:\n","    print(i[0] + ':', i[1])"]},{"cell_type":"code","execution_count":null,"id":"B4idVKkcZgJ8","metadata":{"id":"B4idVKkcZgJ8"},"outputs":[],"source":["# Saving Risks and Probabilities\n","import csv\n","\n","# Risks\n","with open('risks.csv', 'w', newline='') as csvfile:\n","    writer = csv.writer(csvfile)\n","    writer.writerow(['Country', 'Risk', 'Good'])  # Write header\n","    writer.writerows(risk_sorted)  # Write rows\n","\n","# Probabilities\n","with open('probabilities.csv', 'w', newline='') as csvfile:\n","    writer = csv.writer(csvfile)\n","  # Header\n","    writer.writerow(['Country', 'Probability of no child labour', 'Probability of child labour', 'Probability of forced child labour', 'Good'])  # Write header\n","  # Write rows\n","    for i in probability_sorted:\n","      row=[i[0]] + list(i[1]) + [i[2]]\n","      writer.writerow(row)\n","\n"]},{"cell_type":"markdown","source":["## Influence Functions\n","### (Extra - have some limitations since the influence of most instances are really well distributed and at times, the inverse hessian is not computable unless using the partial inverse.)\n","It was seen that, by removing 60 - 80 influencial points, the model resulted in a slighlt higher F1 and accuracy scores (from 64 to 68 in some cases). However, this process needs to be checked by an expert in the field of forced labour to detect why these influential point are so important and wheather they should be removed, tweaked or left in untouched."],"metadata":{"id":"8nTGBepfPIQ0"},"id":"8nTGBepfPIQ0"},{"cell_type":"code","execution_count":null,"metadata":{"id":"ad3FhsV2LR8u"},"outputs":[],"source":["# import numpy as np\n","# import pandas as pd\n","\n","# ## Testing Influence Functions\n","# # Only for Log Regression with sigmoid loss function\n","# class InfluenceFunction(object):\n","#     H_inv = None\n","#     theta_hat = None\n","#     X = None\n","#     y = None\n","#     z = None\n","#     influence_params = []\n","#     influence_loss = []\n","\n","#     def __init__(self, model, X, y):\n","#         self.model = model\n","#         self.theta_hat = model.coef_[0]\n","#         H = self.hessian(X.values, self.theta_hat)\n","#         self.H_inv = np.linalg.inv(H)\n","#         self.X = X\n","#         self.y = y\n","\n","#     def hessian(self, X, theta):\n","#         p = 1 / (1 + np.exp(-X @ theta))\n","#         return (X.T * p * (1 - p)) @ X\n","\n","#     def grad_loss(self, x, y, theta):\n","#         p = 1 / (1 + np.exp(-x @ theta))\n","#         return (p - y) * x\n","\n","#     # I_up,params(z) = -H^(-1) ∇_θ L(z, θ^)\n","#     def influence_function(self):\n","#         self.influence_params = []\n","#         for i in range(len(self.X)):\n","#             x, y_i = self.X.values[i], self.y.iloc[i]\n","#             grad = self.grad_loss(x, y_i, self.theta_hat)\n","#             influence = -self.H_inv @ grad\n","#             self.influence_params.append(influence)\n","#         return self.influence_params\n","\n","#     # I_up,loss(z, z_test) = -∇_θ L(z_test, θ^)^T H^(-1) ∇_θ L(z, θ^)\n","#     def influence_function_test(self, z):\n","#         # Compute the loss gradient at the test point z\n","#         # ---------------\n","#         # # NOTES:\n","#         # # *Positive* influence values indicate that the corresponding training\n","#         # # points have a negative impact on the model's prediction for the test point.\n","#         # # ----\n","#         # # *Negative* influence values contribute to reducing the loss\n","#         # # or improving the model's performance for the given test point.\n","#         # ---------------\n","#         # ∇_θ L(z_test, θ^)^T\n","#         self.z = z\n","#         test_grad = self.grad_loss(z[0], z[1], self.theta_hat).T\n","\n","#         # Compute the influence of each training point\n","#         self.influence_loss = []\n","#         for i in range(len(self.X)):\n","#             x, y_i = self.X.values[i], self.y.iloc[i]\n","#             # ∇_θ L(z, θ^)\n","#             train_grad = self.grad_loss(x, y_i, self.theta_hat)\n","#             influence = -test_grad @ self.H_inv @ train_grad\n","#             self.influence_loss.append(influence)\n","\n","#         return self.influence_loss\n","\n","#     def get_most_influetial_points(self, n=2, asc=False):\n","#         # Append 'Influence_' to each self.X.columns\n","#         ip = pd.DataFrame(self.influence_params, columns=['Influence_' + str(i+1) for i in range(len(self.X.columns))])\n","#         total_n = len(ip)\n","\n","#         # Add columns for the training points\n","#         for i in range(len(self.X.columns)):\n","#             ip[f'X_{i+1}'] = self.X[self.X.columns[i]]\n","\n","#         ip['influence_sum'] = ip[['Influence_' + str(i+1) for i in range(len(self.X.columns))]].abs().sum(axis=1)\n","\n","#         # Calculate Aprox_param_i = θ^ - I_up,params(z) / n\n","#         # ip = pd.merge(ip, columns=['Aprox_param_' + self.X.columns[i] for i in range(len(self.X.columns))])\n","#         for i in range(len(self.X.columns)):\n","#             ip[f'Aprox_param_{i+1}'] = self.theta_hat[i] - (ip[\"Influence_\" + str(i+1)] / total_n)\n","\n","\n","#         ip.sort_values(by='influence_sum', ascending=asc, inplace=True)\n","#         return ip.head(n)\n","\n","#     def get_most_influetial_points_test(self, n=2, positives=True):\n","#         ip = pd.DataFrame(self.influence_loss, columns=['loss'])\n","#         # Add columns for the training points\n","#         for i in range(len(self.X.columns)):\n","#             ip[f'X_{i+1}'] = self.X[self.X.columns[i]]\n","\n","#         ip.sort_values(by='loss', ascending=positives, inplace=True)\n","#         return ( ip.head(n), self.z )"],"id":"ad3FhsV2LR8u"},{"cell_type":"code","execution_count":null,"metadata":{"id":"Odt6dknFLR8u"},"outputs":[],"source":["# ifunc = InfluenceFunction(ordinal_logistic, x_train_resampled, y_train_resampled)\n","# inf = ifunc.influence_function()\n","# # test_point = x_train.iloc[13]\n","# # test_label = y_train[13]\n","# # inf_z = ifunc.influence_function_test((test_point, test_label))\n","# # print(pd.DataFrame(inf, columns=x_train.columns))\n","# # print('---------------')\n","# # print(pd.DataFrame(inf_z, columns=['Influence_Loss_zTest']))\n","# mip = ifunc.get_most_influetial_points(n=20)\n","# mip"],"id":"Odt6dknFLR8u"},{"cell_type":"code","execution_count":null,"metadata":{"id":"fBY7ezdgLR8u"},"outputs":[],"source":["# mip = ifunc.get_most_influetial_points(n=60)\n","# indexes = mip.index.values\n","\n","# # remove the most influential points (indexes) form x_train\n","# x_train_2 = x_train_resampled.drop(indexes)\n","# y_train_2 = y_train_resampled.drop(indexes)\n","\n","\n","# # Train the ordinal logistic regression model\n","# ordinal_logistic = LogisticRegression(solver='saga', max_iter=5000, C=1, multi_class='multinomial')\n","# ordinal_logistic.fit(x_train_2, y_train_2)\n","\n","\n","# # Test the model\n","# y_pred_log = ordinal_logistic.predict(x_test)\n","# y_tpred_log = ordinal_logistic.predict(x_train)\n","# acc_test1 = accuracy_score(y_test, y_pred_log)\n","# acc_train1 = accuracy_score(y_train, y_tpred_log)\n","\n","# f1_test = f1_score(y_test, y_pred_log, average='weighted')\n","# print(\"Ordinal Logistic Regression Test F1:\", f1_test)\n","\n","# ## Accuracy\n","# print(\"Ordinal Logistic Regression Test Accuracy:\", acc_test1)\n","\n","# # plot training and test accuracy\n","# import matplotlib.pyplot as plt\n","\n","# # Create a list of model names\n","# model_names = ['Ordinal Logistic Regression', 'SVM', 'Random Forest', 'AdaBoost', 'Neural Network']\n","\n","# # Create lists for train and test accuracies\n","# train_accuracies = [acc_train1, acc_train2, acc_train3, acc_train4, acc_train5]\n","# test_accuracies = [acc_test1, acc_test2, acc_test3, acc_test4, acc_test5]\n","\n","# # Create a figure and axis object\n","# fig, ax = plt.subplots(figsize=(10, 6))\n","\n","# # Set the x-axis tick positions and labels\n","# x = np.arange(len(model_names))\n","# ax.set_xticks(x)\n","# ax.set_xticklabels(model_names, rotation=45, ha='right')\n","\n","# # Plot the train and test accuracies\n","# bar_width = 0.35\n","# rects1 = ax.bar(x - bar_width / 2, train_accuracies, bar_width, label='Train Accuracy')\n","# rects2 = ax.bar(x + bar_width / 2, test_accuracies, bar_width, label='Test Accuracy')\n","\n","# # Add labels and legend\n","# ax.set_ylabel('Accuracy')\n","# ax.set_title('Train vs Test Accuracy Comparison')\n","# ax.legend()\n","\n","# # Add value labels on top of the bars\n","# for rect1, rect2, model in zip(rects1, rects2, model_names):\n","#     ax.text(rect1.get_x() + rect1.get_width() / 2, rect1.get_height() + 0.01, f'{rect1.get_height():.2f}', ha='center', va='bottom')\n","#     ax.text(rect2.get_x() + rect2.get_width() / 2, rect2.get_height() + 0.01, f'{rect2.get_height():.2f}', ha='center', va='bottom')\n","\n","# # Adjust layout and display the plot\n","# plt.tight_layout()\n","# plt.show()"],"id":"fBY7ezdgLR8u"},{"cell_type":"code","execution_count":null,"metadata":{"id":"9i7s78L0LR8u"},"outputs":[],"source":["# unique_values, counts = np.unique(y_pred_log, return_counts=True)\n","# print(unique_values, counts)\n","\n","# unique_values, counts = np.unique(y_test, return_counts=True)\n","# print(unique_values, counts)"],"id":"9i7s78L0LR8u"}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":5}